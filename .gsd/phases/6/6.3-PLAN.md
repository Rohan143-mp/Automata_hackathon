---
phase: 6
plan: 3
wave: 1
---

# Plan 6.3: Orchestration with docker-compose

## Objective
Combine all independent containerized services (Backend, PC Hub, and Ollama) into a unified `docker-compose.yml` network so that they can communicate internally and start with one command.

## Context
- .gsd/ROADMAP.md
- docker-compose.yml (if exists)

## Tasks

<task type="auto">
  <name>Create System `docker-compose.yml`</name>
  <files>docker-compose.yml</files>
  <action>
    - Ensure the `docker-compose.yml` defines the following services:
      - `ollama`: Pulls `ollama/ollama`, exposes port `11434`, and mounts a local volume `/root/.ollama` to persist models. Let this run a command on boot: `ollama pull llama3` (or tinyllama).
      - `hub`: Builds the `./hub` Dockerfile. Expose `81`, `82`, `8765`. Attach to the Ollama network `depends_on: ollama`.
      - `backend`: Builds the `./backend` Dockerfile. Expose `3000`.
    - Set environment variables for the hub so it points to `http://ollama:11434` instead of `localhost`. 
  </action>
  <verify>cat docker-compose.yml</verify>
  <done>Docker-compose defines the full stack for the Neural-Intent engine.</done>
</task>

## Success Criteria
- [ ] Running `docker-compose up` boots all dependencies for the Bio-Digital Larynx in a single stroke.
