---
phase: 6
plan: 4
wave: 2
---

# Plan 6.4: GPU Acceleration for PC Hub

## Objective
The Whisper audio transcription and Ollama inference are heavily computationally intensive. Providing GPU passthrough to the containers ensures the Neural-Intent engine retains its sub-second latency constraint.

## Context
- .gsd/ROADMAP.md
- docker-compose.yml (From Plan 6.3)

## Tasks

<task type="auto">
  <name>Configure NVIDIA Container Toolkit Passthrough</name>
  <files>docker-compose.yml</files>
  <action>
    - Ensure the `docker-compose.yml` specifies `deploy` and `resources` logic for both the `hub` and the `ollama` services.
    - Set `reservations` to accept `devices` with `capabilities: [gpu]`.
    - Note that the host running this compose MUST have the NVIDIA drivers and toolkit installed.
  </action>
  <verify>cat docker-compose.yml</verify>
  <done>Docker-compose specifies GPU requirements for inference operations.</done>
</task>

## Success Criteria
- [ ] Deploying the compose stack takes advantage of hardware VRAM when transcribing Whisper audio and generating text.
