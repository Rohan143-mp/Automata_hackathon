---
phase: 3
plan: 2
wave: 2
---

# Plan 3.2: Ollama Intent Service & Context Logic

## Objective
Translate the raw gesture intent (e.g., "question") into a fluent, contextualized sentence using a local LLM and recent Whisper classroom audio transcripts.

## Context
- .gsd/SPEC.md
- hub/main_server.py (from Plan 3.1)

## Tasks

<task type="auto">
  <name>Build Ollama Intent Service</name>
  <files>hub/services/intent_service.py</files>
  <action>
    - Ensure `requests` is installed for Python HTTP calls.
    - Create a function `generate_fluent_speech(gesture_intent, recent_transcript)`.
    - Function must POST to `http://localhost:11434/api/generate` (Ollama default port). Use a lightweight model like `llama3` or `phi3`.
    - Construct a System Prompt: "You are the voice of a student in a classroom. They just signed: '{gesture_intent}'. The teacher recently said: '{recent_transcript}'. Generate a single, natural sentence the student would say. Do not add quotes or explanations."
    - Return the generated string.
  </action>
  <verify>python -m py_compile hub/services/intent_service.py</verify>
  <done>Python script successfully structures the HTTP request to a local Ollama instance.</done>
</task>

<task type="auto">
  <name>Integrate Intent Engine into Main Server</name>
  <files>
    hub/main_server.py
    hub/services/whisper_service.py (Mock version)
  </files>
  <action>
    - Create a mock `get_recent_transcript()` in a temporary `whisper_service.py` that just returns a hardcoded string like "The teacher is explaining photosynthesis." (Phase 1's true Whisper integration will be wired in Phase 5).
    - Update `main_server.py`. When `match_gesture()` returns an intent, immediately pass it to `generate_fluent_speech(intent, mock_transcript)`.
    - Print the final generated phrase to the console: `[Speech Output]: "Excuse me, Professor. I have a question about photosynthesis."`
  </action>
  <verify>python -m py_compile hub/main_server.py</verify>
  <done>Main hub gracefully routes a matched vector to Ollama and retrieves the contextualized sentence.</done>
</task>

## Success Criteria
- [ ] Gesture intents are grammatically expanded using LLM context logic.
- [ ] Hub logic is completely modular (engine -> intent service).
